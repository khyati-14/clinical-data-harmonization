{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0211e695-49b5-48ad-a840-14483f2f387d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(trino://pulkit.agrawal%40grofers.com@adhoc-trino.analytics.blinkit.in:443/blinkit/default)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pencilbox as pb\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5dae181-9aa4-4193-a5fb-0a09fc2ba5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Collecting et-xmlfile\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad52ef0-dbfe-41fb-86ab-691dfc01ae31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Harmonizing Clinical Concepts\n",
    "\n",
    "## Repository Overview\n",
    "This repository contains all the necessary files and instructions required for this problem statement on harmonizing clinical concepts.\n",
    "\n",
    "## Contents of the Zip Folder\n",
    "- **`Test.xlsx`**  \n",
    "  Excel file containing input data. Your predictions will be appended to this file.\n",
    "\n",
    "- **`snomed_all_data.parquet`**  \n",
    "  Parquet file containing all target codes and descriptions for the SNOMED CT coding system.\n",
    "\n",
    "- **`rxnorm_all_data.parquet`**  \n",
    "  Parquet file containing all target codes and descriptions for the RxNorm coding system.\n",
    "\n",
    "- **`Column Reference Guide.md`**  \n",
    "  Markdown file explaining the columns present in the SNOMED CT and RxNorm parquet files.\n",
    "\n",
    "## Submission Guidelines\n",
    "To complete your submission:\n",
    "\n",
    "1. **Modify the `Test.xlsx` file** by appending the following three columns:\n",
    "   - `Output Coding System`: Either `SNOMEDCT_US` or `RXNORM`\n",
    "   - `Output Target Code`: Predicted code in **string** format\n",
    "   - `Output Target Description`: Human-readable description of the predicted code\n",
    "\n",
    "2. **Your solution in a public code repository.**\n",
    "\n",
    "3. **Ensure your repository includes:**\n",
    "   - A clear, concise `README.md` explaining:\n",
    "     - Your technical solution\n",
    "     - A short walkthrough of the repository\n",
    "   - Code that can be easily tested with additional input files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606ae4a-dae3-41bd-a39d-297b9688ea1a",
   "metadata": {},
   "source": [
    "# Columns Reference Guide\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `CUI` — Concept Unique Identifier\n",
    "- **Definition**: A unique identifier assigned to a concept.  \n",
    "- **Purpose**: Groups together all terms from different vocabularies (SNOMED CT, RxNorm, ICD, etc.) that mean the same thing (are synonymous).  \n",
    "- **Format**: Always starts with a `\"C\"` followed by 7 digits (e.g., `C0011849`).  \n",
    "- **Example**:\n",
    "  - `C0011849` → *Diabetes Mellitus*  \n",
    "    - In RxNorm: `Diabetes Mellitus`  \n",
    "    - In SNOMED CT: `44054006 | Diabetes mellitus (disorder) |`\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `System` — Source Vocabulary (SAB)\n",
    "- **Definition**: Abbreviation for the source vocabulary or terminology that contributed the concept.  \n",
    "- **Purpose**: Identifies where the term came from (RxNorm, SNOMED CT, MeSH, ICD-10, etc.).  \n",
    "- **Example**:\n",
    "  - `RXNORM` → concepts from the RxNorm vocabulary.  \n",
    "  - `SNOMEDCT_US` → concepts from the U.S. edition of SNOMED CT.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. `TTY` — Term Type\n",
    "- **Definition**: Describes the role of the term within the source vocabulary.  \n",
    "- **Purpose**: Differentiates between preferred names, synonyms, codes, ingredients, brand names, etc.  \n",
    "- **Common Values**:\n",
    "  - `PT` (SNOMED CT) → Preferred Term.  \n",
    "  - `SY` (RxNorm, SNOMED CT) → Synonym.  \n",
    "  - `SCD` (RxNorm) → Semantic Clinical Drug (normalized clinical drug concept).  \n",
    "  - `BN` (RxNorm) → Brand Name.  \n",
    "- **Example**:\n",
    "  - RxNorm:  \n",
    "    - `SCD` → *Metformin 500 MG Oral Tablet*  \n",
    "    - `BN` → *Glucophage*  \n",
    "  - SNOMED CT:  \n",
    "    - `PT` → *Diabetes mellitus (disorder)*  \n",
    "    - `SY` → *Sugar diabetes*  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. `CODE` — Source Code\n",
    "- **Definition**: The identifier used **within the source vocabulary** for the concept.  \n",
    "- **Purpose**: Provides the original vocabulary-specific code.  \n",
    "- **Format**: Varies by vocabulary.  \n",
    "- **Example**:\n",
    "  - RxNorm: `860975` → *Metformin 500 MG Oral Tablet*  \n",
    "  - SNOMED CT: `44054006` → *Diabetes mellitus (disorder)*  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. `STR` — String (Term Name)\n",
    "- **Definition**: The actual human-readable name of the concept as it appears in the source vocabulary.  \n",
    "- **Purpose**: Stores the textual representation of the concept.  \n",
    "- **Example**:\n",
    "  - RxNorm: `Metformin 500 MG Oral Tablet`  \n",
    "  - SNOMED CT: `Diabetes mellitus (disorder)`  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. `STY` — Semantic Type\n",
    "- **Definition**: The broad category or semantic group.  \n",
    "- **Purpose**: Provides higher-level grouping of concepts beyond source vocabularies.  \n",
    "- **Examples**:\n",
    "  - `T047` → *Disease or Syndrome*  \n",
    "  - `T121` → *Pharmacologic Substance*  \n",
    "- **Example**:\n",
    "  - `C0011849` (*Diabetes Mellitus*) → `Disease or Syndrome`  \n",
    "  - `C0025598` (*Metformin*) → `Pharmacologic Substance`  \n",
    "\n",
    "---\n",
    "\n",
    "# Summary Table\n",
    "\n",
    "| Column  | Meaning                                | Example (RxNorm)                   | Example (SNOMED CT)                    |\n",
    "|---------|----------------------------------------|------------------------------------|----------------------------------------|\n",
    "| `CUI`   | Concept Unique Identifier              | `C0025598` (*Metformin*)           | `C0011849` (*Diabetes Mellitus*)       |\n",
    "| `System`| Source vocabulary abbreviation (SAB)   | `RXNORM`                           | `SNOMEDCT_US`                          |\n",
    "| `TTY`   | Term type in source vocabulary         | `SCD` (Semantic Clinical Drug)     | `PT` (Preferred Term)                  |\n",
    "| `CODE`  | Code in source vocabulary              | `860975`                           | `44054006`                             |\n",
    "| `STR`   | Human-readable string/term             | *Metformin 500 MG Oral Tablet*     | *Diabetes mellitus (disorder)*         |\n",
    "| `STY`   | Semantic type                          | *Pharmacologic Substance*          | *Disease or Syndrome*                  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918681e7-cc7b-4652-9f8e-258332423c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('rxnorm_all_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51cf29c6-983f-43b5-91ae-c6fafc2b211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_parquet(\"snomed_all_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c29c8c-3a5c-4ece-9c2f-7258a9351737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(\"Test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e59374-d710-4e71-b396-1183375dc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thefuzz\n",
      "  Using cached thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0\n",
      "  Using cached rapidfuzz-3.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: rapidfuzz, thefuzz\n",
      "Successfully installed rapidfuzz-3.13.0 thefuzz-0.22.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec885442-d9be-4aee-8c5f-e0f262f817db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data files loaded successfully!\n",
      "Applying cleaning pipeline to all text data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from thefuzz import fuzz\n",
    "import re\n",
    "import time\n",
    "\n",
    "# --- 1. Load Knowledge Base from External Files ---\n",
    "\n",
    "def load_correction_map(filepath=\"correction_map.txt\"):\n",
    "    \"\"\"Loads the correction map from a text file.\"\"\"\n",
    "    correction_map = {}\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                if ':' in line:\n",
    "                    key, value = line.strip().split(':', 1)\n",
    "                    correction_map[key.strip()] = value.strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: '{filepath}' not found. The cleaner will run without a correction map.\")\n",
    "    return correction_map\n",
    "\n",
    "def load_redundant_keywords(filepath=\"redundant_keywords.txt\"):\n",
    "    \"\"\"Loads the list of redundant keywords from a text file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: '{filepath}' not found. The cleaner will run without removing keywords.\")\n",
    "        return []\n",
    "\n",
    "# --- 2. Advanced Cleaning Pipeline (Now driven by external files) ---\n",
    "\n",
    "def create_cleaner(correction_map, redundant_keywords):\n",
    "    \"\"\"Creates a cleaning function based on the loaded knowledge base.\"\"\"\n",
    "    def clean_and_standardize(text):\n",
    "        if not isinstance(text, str): return \"\"\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Remove instructions, dosages, etc.\n",
    "        text = re.sub(r'sig:.*', '', text)\n",
    "        text = re.sub(r'take \\d+(\\s*\\(.*\\))? tablet\\(s\\)?', '', text)\n",
    "        text = re.sub(r'\\d+(\\.\\d+)?\\s*(mg|ml|mcg|unit|units|g|gram|grams)\\b', '', text)\n",
    "        text = re.sub(r'\\b(by|via|every|with|as needed|at bedtime|oral|route|injection|topically)\\b', '', text)\n",
    "        \n",
    "        # Apply corrections from the loaded map\n",
    "        for key, value in sorted(correction_map.items(), key=lambda item: len(item[0]), reverse=True):\n",
    "            text = re.sub(r'\\b' + re.escape(key) + r'\\b', value, text)\n",
    "            \n",
    "        # Remove redundant keywords from the loaded list\n",
    "        for word in redundant_keywords:\n",
    "            text = re.sub(r'\\b' + word + r'\\b', '', text)\n",
    "            \n",
    "        # Final cleanup\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    return clean_and_standardize\n",
    "\n",
    "# --- 3. Load Data and Initialize Cleaner ---\n",
    "try:\n",
    "    test_df = df2\n",
    "    snomed_df = df1\n",
    "    rxnorm_df = df\n",
    "    print(\"All data files loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure all required CSV files are in the directory.\")\n",
    "    exit()\n",
    "\n",
    "# Load the knowledge base and create the cleaning function\n",
    "correction_map = load_correction_map()\n",
    "redundant_keywords = load_redundant_keywords()\n",
    "cleaner = create_cleaner(correction_map, redundant_keywords)\n",
    "\n",
    "print(\"Applying cleaning pipeline to all text data...\")\n",
    "test_df['Cleaned_Description'] = test_df['Input Entity Description'].apply(cleaner)\n",
    "snomed_df['Cleaned_STR'] = snomed_df['STR'].apply(cleaner)\n",
    "rxnorm_df['Cleaned_STR'] = rxnorm_df['STR'].apply(cleaner)\n",
    "\n",
    "# --- 4. Two-Stage Harmonization (Unchanged) ---\n",
    "def find_best_match(description, entity_type, snomed_data, rxnorm_data, snomed_tfidf, rxnorm_tfidf, snomed_vectorizer, rxnorm_vectorizer):\n",
    "    is_snomed = entity_type in ['Procedure', 'Lab', 'Diagnosis']\n",
    "    data, tfidf_matrix, vectorizer, system = (snomed_data, snomed_tfidf, snomed_vectorizer, 'SNOMEDCT_US') if is_snomed else (rxnorm_data, rxnorm_tfidf, rxnorm_vectorizer, 'RXNORM')\n",
    "    \n",
    "    if not description.strip():\n",
    "        return {'System': system, 'Code': 'NO_INPUT', 'Description': 'Original text was empty after cleaning'}\n",
    "\n",
    "    desc_tfidf = vectorizer.transform([description])\n",
    "    similarities = cosine_similarity(desc_tfidf, tfidf_matrix).flatten()\n",
    "    candidate_indices = similarities.argsort()[-20:][::-1]\n",
    "    \n",
    "    if len(candidate_indices) == 0 or similarities[candidate_indices[0]] < 0.1:\n",
    "        return {'System': system, 'Code': 'NOT_FOUND', 'Description': 'No suitable match found'}\n",
    "\n",
    "    best_final_score, best_match_index = -1, -1\n",
    "    input_words = set(description.split())\n",
    "    \n",
    "    for idx in candidate_indices:\n",
    "        candidate_str = data.iloc[idx]['Cleaned_STR']\n",
    "        wratio_score = fuzz.WRatio(description, candidate_str)\n",
    "        candidate_words = set(candidate_str.split())\n",
    "        overlap_score = (len(input_words.intersection(candidate_words)) / len(input_words)) * 100 if input_words else 0\n",
    "        final_score = (wratio_score * 0.4) + (overlap_score * 0.6)\n",
    "        \n",
    "        if final_score > best_final_score:\n",
    "            best_final_score, best_match_index = final_score, idx\n",
    "\n",
    "    return {'System': system, 'Code': str(data.iloc[best_match_index]['CODE']), 'Description': data.iloc[best_match_index]['STR']}\n",
    "\n",
    "# --- 5. Execution ---\n",
    "print(\"Preparing TF-IDF matrices...\")\n",
    "snomed_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "snomed_tfidf = snomed_vectorizer.fit_transform(snomed_df['Cleaned_STR'].fillna(''))\n",
    "rxnorm_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "rxnorm_tfidf = rxnorm_vectorizer.fit_transform(rxnorm_df['Cleaned_STR'].fillna(''))\n",
    "\n",
    "print(\"Running harmonization process...\")\n",
    "start_time = time.time()\n",
    "results = []\n",
    "for index, row in test_df.iterrows():\n",
    "    match = find_best_match(row['Cleaned_Description'], row['Entity Type'], snomed_df, rxnorm_df, snomed_tfidf, rxnorm_tfidf, snomed_vectorizer, rxnorm_vectorizer)\n",
    "    results.append({'Input Entity Description': row['Input Entity Description'], 'Entity Type': row['Entity Type'], 'Output Coding System': match['System'], 'Output Target Code': match['Code'], 'Output Target Description': match['Description']})\n",
    "\n",
    "print(f\"Harmonization finished in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 6. Final Output ---\n",
    "final_df = pd.DataFrame(results)\n",
    "output_filename = 'final.csv'\n",
    "final_df.to_csv(output_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce573ccb-5fd3-41c3-9b7e-c4e3847866b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
